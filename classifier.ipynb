{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from models.FC_models import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## helpers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def imsave(img, name, path, saving_path=None):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    #saving_path = os.path.join(args.output_path, name+'.png')\n",
    "    print(f\"saving to {saving_path }\")\n",
    "    torchvision.utils.save_image(img, saving_path)\n",
    "\n",
    "def imshow(img, name, ax=None):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    if ax:\n",
    "        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATCH_SIZE = 8\n",
    "\n",
    "model_path = \"saved_models/mikmik_fc_res-p8_b16-1_100.pkl\"\n",
    "model = Residual2CoreFC(coded_size=16,\n",
    "                        patch_size=PATCH_SIZE,\n",
    "                        num_passes=4)\n",
    "model.load_state_dict(torch.load(model_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Normalize function so given an image of range [0, 1] transforms it into a Tensor range [-1. 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the CIFAR LOADER\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "dataiter = iter(train_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# produce reconstructed images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(99)\n"
     ]
    }
   ],
   "source": [
    "from evaluation import to_patches, reconstruct_patches\n",
    "\n",
    "imgs, labels = next(dataiter)\n",
    "print(labels[0])\n",
    "\n",
    "#\n",
    "def reconstruct(imgs, model = model):\n",
    "    # break to patches\n",
    "    patches = to_patches(imgs, PATCH_SIZE)\n",
    "    r_patches = [model.sample(p) for p in patches] # pass all the patches\n",
    "    reconstructed = reconstruct_patches(r_patches)\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "reconstructed = reconstruct(imgs,model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show image alongside original\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "imshow(reconstructed[0], \"reconstructed\", ax=axarr[0])\n",
    "imshow(imgs[0], \"original\", ax=axarr[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = torchvision.models.resnet18(num_classes=1000, weights=torchvision.models.ResNet18_Weights.DEFAULT)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = \"cpu\"\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "              nn.BatchNorm2d(out_channels),\n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "\n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        self.conv5 = conv_block(512, 1028, pool=True)\n",
    "        self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
    "                                        nn.Flatten(), # 1028\n",
    "                                        nn.Linear(1028, num_classes)) # 1028 -> 100\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.conv5(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "classifier = to_device(ResNet9(3, 100), device)\n",
    "classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "#model = resnet100(pretrained=False)  # Set pretrained=False since we are loading custom weights\n",
    "\n",
    "# Update this number of output features to match the CIFAR-100 classes if not using ResNet50 directly or the model has been modified\n",
    "#model.fc = torch.nn.Linear(model.fc.in_features, 100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "# Load the weights\n",
    "classifier.load_state_dict(torch.load('classifiers/group22_pretrained_model.h5',map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imgs1, labels = next(dataiter)\n",
    "print(labels[0])\n",
    "print(f\"predict from origin: {classifier(imgs1).argmax()}\")\n",
    "\n",
    "reconstructed = reconstruct(imgs1,model)\n",
    "print(f\"predicted from reconstruction: {classifier(reconstructed).argmax()}\")\n",
    "\n",
    "# show image alongside original\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "imshow(reconstructed[0], \"reconstructed\", ax=axarr[0])\n",
    "imshow(imgs1[0], \"original\", ax=axarr[1])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reconstruct_stages(imgs, model = model):\n",
    "    # break to patches\n",
    "    patches = to_patches(imgs, PATCH_SIZE)\n",
    "\n",
    "    r_patches = [model.get_sampled_steps(p) for p in patches] # pass all the patches\n",
    "    print(len(r_patches[0]))\n",
    "    reconstructed = []\n",
    "    for i in range(len(r_patches[0])):\n",
    "        reconstructed.append( reconstruct_patches([r[i] for r in r_patches]))\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "reconstructed = reconstruct(imgs,model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stages = reconstruct_stages(imgs1)\n",
    "\n",
    "f, axarr = plt.subplots(1, 5)\n",
    "# imshow(reconstructed[0], \"reconstructed\", ax=axarr[0])\n",
    "imshow(imgs1[0], \"original\", ax=axarr[0])\n",
    "\n",
    "for i, stage in enumerate(stages):\n",
    "    #reconstructed = reconstruct_patches(stage)\n",
    "    imshow(stage[0], \"original\", ax=axarr[i+1])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
